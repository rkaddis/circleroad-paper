\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
%Template version as of 6/27/2024

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}


% fix for unaligned author blocks
% from https://tex.stackexchange.com/questions/458204/ieeetran-document-class-how-to-align-five-authors-properly/458208#458208

\makeatletter
\newcommand{\linebreakand}{%
    \end{@IEEEauthorhalign}
    \hfill\mbox{}\par
    \mbox{}\hfill\begin{@IEEEauthorhalign}
}
\makeatother

\title{Evolutionary Hyperparameter Optimization to Find Lightweight CNN Models for Autonomous Steering}

\author{\IEEEauthorblockN{Devson Butani}
    \IEEEauthorblockA{\textit{Department of Math and Computer Science} \\
        \textit{Lawrence Technological University}\\
        Southfield, MI, USA \\
        dbutani@ltu.edu}
    \and
    \IEEEauthorblockN{Ryan Kaddis}
    \IEEEauthorblockA{\textit{Department of Math and Computer Science} \\
        \textit{Lawrence Technological University}\\
        Southfield, MI, USA \\
        rkaddis@ltu.edu}
    \linebreakand
    \IEEEauthorblockN{Chan-Jin Chung}
    \IEEEauthorblockA{\textit{Department of Math and Computer Science} \\
        \textit{Lawrence Technological University}\\
        Southfield, MI, USA \\
        cchung@ltu.edu}
}

\maketitle

\begin{abstract}
    This research investigates the optimization of Convolutional Neural Networks (CNNs) for autonomous steering using the (N+M) Evolutionary Strategy (ES) with the 1/5th success rule. The primary objective is to develop a lightweight CNN model capable of real-time steering angle prediction, mimicking human driving behavior on predefined paths. The ES algorithm automates hyperparameter tuning, dynamically adjusting parameters such as filter sizes and layer configurations.

    Data collection encompasses driving scenarios recorded via the LTU ACTor autonomous driving platform, including variations in path direction and driving style. The dataset consists of timestamped images labeled with steering angles, pre-processed to focus on relevant visual information.

    Initial experiments involve training a baseline CNN model, which is then refined using ES to significantly reduce model size while maintaining competitive predictive accuracy. The results highlight the viability of lightweight CNN architectures for real-time autonomous systems, striking a balance between computational efficiency and performance. This study not only advances research initiatives in using evolutionary strategies for autonomous driving applications but also lays the groundwork for deploying cost-effective, scalable solutions in self-driving technology.
\end{abstract}

\begin{IEEEkeywords}
    Evolutionary strategy, Hyperparameter optimization, Convolutional neural networks, Lightweight CNN, Autonomous driving, Deep learning, Real-time performance, Autonomous vehicles, Cameras
\end{IEEEkeywords}

\section{Introduction}
Autonomous driving systems require robust models capable of making real-time decisions under varying conditions. A critical component of these systems is the prediction of steering angles based on camera input, which involves processing visual data effectively and efficiently on compute-limited on-board processors \cite{PilotNET_application}. This research focuses on training and optimizing CNNs leveraging evolutionary strategies (ES) to automate the search for the best-performing and best size-reduced model configurations.

This research directly addresses the limitations that come with larger models and limited data. Larger models, while often achieving higher accuracy, come with increased computational cost and memory requirements, which can lead to slower inference times and higher energy consumption \cite{self_drive_latency_and_model_size_drawbacks}. This poses a significant challenge for real-time applications like autonomous driving, where split-second decisions and energy efficiency are crucial. The size of a model directly impacts its deployment feasibility, particularly in resource-constrained on-board processors \cite{MobileNET_size_and_speed}. Minimizing model size while also training with limited data is the core design challenge in autonomous driving systems.

Furthermore, the performance of deep learning models is heavily reliant on the availability of large, diverse, and labeled datasets. In the context of autonomous driving, collecting and annotating such datasets for every conceivable scenario is impractical and cost-prohibitive \cite{ImageNET_large_scale_database} Even though limited data can lead to overfitting, where the model performs well on the training data but fails to generalize to unseen scenarios or conditions, this is often not a problem in real-world applications where the operational design domain (i.e., the area of interest) can be regulated. For example, deploying an autonomous vehicle specifically for a small city or a specific set of routes. With this method, we can leverage adding small datasets everyime the domain expands and utilize techniques like transfer learning and few shot learning to improve model performance over time \cite{few_shot_learning}. While optimization techniques have been extensively explored in various domains, their application to creating lightweight and adaptable models for autonomous driving, specifically for real-time steering angle prediction, remains an active area of research.

Evolutionary Strategies (ES) offer a compelling advantage in this context. ES is a population-based optimization algorithm that leverages the collective intelligence of a population of candidate solutions to find the best-performing solution within a given search space \cite{ES_introduction}. Unlike gradient-based optimization methods, which can struggle in complex, non-differentiable, or noisy search spaces, ES algorithms are well-suited for exploring such landscapes \cite{ES_scalable}. This is particularly relevant to hyperparameter optimization of CNNs for autonomous driving, where the relationship between model architecture, hyperparameters, and performance can be highly complex and non-linear.

\subsection{Data Acquisition and Preprocessing}

\textbf{Equipment:} The LTU ACTor autonomous driving platform, integrated with the Robot Operating System (ROS), was used for data collection. Sensor data, including images from the vehicle's forward-facing camera and corresponding steering angles, were recorded in rosbag files. This setup ensured synchronized visual and control data, essential for training and evaluating the models.

\textbf{Environment:} Data was collected along a predefined circular path: the red brick path surrounding Ockham's Wedge at LTU. To introduce variability and enhance model robustness, driving sessions included clockwise and counterclockwise directions, smooth and zigzag maneuvers, and driving along inner and outer path edges to simulate diverse spatial alignments.  Figure X shows an overhead view of the data collection site.

Figure...

\textbf{Extraction and Preprocessing:} A custom script was developed to extract and preprocess data from rosbag files. Images were saved at 200 ms intervals, with filenames encoding timestamps and steering angles. Each image was cropped to exclude irrelevant regions, such as the sky and surrounding buildings. The resulting dataset consisted of 2,957 images, split into 70\% training (2,069), 20\% validation (592), and 10\% testing (296).

\subsection{Research Goals}

The primary goals of this research are:
\begin{enumerate}
    \item Develop a framework to apply the (N+M) Evolutionary Strategy (ES) with the 1/5th success rule for automated hyperparameter tuning.
    \item Minimize the size of the baseline CNN model while maintaining satisfactory steering angle prediction performance.
    \item Validate the real-world applicability of ES-optimized models in autonomous vehicle systems.
\end{enumerate}

\section{Methodology}

Model Training and Establishing a Baseline Model
Models are trained using the Keras API for Python on top of PyTorch backend to leverage GPU compute capacity. The models were trained on our dataset using the Mean Squared Error (MSE) loss function, which quantifies the average squared difference between predicted and actual steering angles. Lower MSE value and lower MAE value both indicate better performance. This serves as a benchmark for evaluating the improvements introduced by the evolution strategy (ES) with 1/5 success rule optimized models.

We began with a single-layer CNN as a simple baseline model. However, it quickly became evident that such a simplistic architecture lacked the capacity to produce results suitable for real-world applications. To address this, we adopted PilotNet—a pre-designed CNN developed by NVIDIA \cite{PilotNET}. PilotNet, an early milestone in autonomous steering research, demonstrated the potential of GPU-accelerated deep learning for this domain.

Additionally, early stopping was added to stop the training instances after 4 epochs with no improvement in mean squared error metric. This prevents hyperparameters with no impact towards model improvement from consuming time and processing power.

Dataset and Hyperparameter Management
The dataset was randomly divided into 70\% training, 20\% validation, and 10\% testing splits to ensure sufficient data for model evaluation. Initially, hyperparameters included individual layer units, batch sizes, learning rates, activation functions, and optimizer selection. The layer units ranged from 20\% of the baseline to 300\% as baseline to allow for a wide range of optimization in each layer.

However, after initial experiments, it was observed that reducing the optimization scope to focus solely on layer units significantly improved performance and reduced computation time. Consequently, batch size (32), learning rate (0.001), activation function (ReLU), and optimizer (Adam) were fixed to match the baseline model settings.

Mention HP ranges controlled...

Optimization Framework
The CNN hyperparameters—such as the number of filters in convolutional layers and the number of neurons in fully connected layers—were encoded as genes in an evolutionary strategy (ES) framework. Specifically, we employed the (N+M)-ES approach, which iteratively optimizes hyperparameters by combining the N best parent models and M offspring models generated through mutations.


\subsection{ES Algorithm}
The process proceeds as follows:
Initialization: Generate and train N parent models with random hyperparameters within predefined ranges.
Child Generation: Create M offspring by mutating parent hyperparameters with:

\begin{equation}
    \hat{h} = h + \text{gauss}(\sigma * (\text{max}(h) - \text{min}(h)))
\end{equation}


where h represents a hyperparameter and $\sigma$ is the step size
Training and Evaluation: Train all child models and evaluate their validation loss and accuracy.
Selection: Retain the top N models (from parents and children) based on validation loss and accuracy to form the next generation's parents.
This process iterates until the mutation step size ($\sigma$) is dynamically optimized using the 1/5 success rule \cite{ES_one_fifth_rule}:

\begin{equation}
    \sigma_{t+1} = \left\{
    \begin{array}{ll}
        \alpha * \sigma_t & \text{success rate} > \frac{1}{5}    \\
        \beta * \sigma_t  & \text{success rate} \leq \frac{1}{5} \\
    \end{array}
    \right.
\end{equation}


where $\alpha$ increases the step size, $\beta$ decreases it based on success rate—the fraction of offspring outperforming their parents. This ensures efficient exploration of the hyperparameter space while avoiding local minima.Computational Resources: Training was performed on Lawrence Technological University’s NVIDIA A100 GPU server, enabling efficient parallel training of N parents and M offsprings model pools.

Using a larger pool of parents and offsprings allows for the algorithm to explore the solution space nearby the current parameters. The larger N and M are, the faster the evolutionary search can find a viable solution. This enables an increase in N and M values as model size decreases. The 80GB VRAM of this GPU allows using higher N and M values to balance the tradeoff between exploration and exploitation.

\subsection{Model Testing}
Add paragraph detailing the real-time testing process using Gazelle Sim. Also describe how the pipeline works. References to use \cite{CNN_can_self_drive}, \cite{lane_detection_good_results_zigzag}, \cite{KITTI_dataset}.

\section{Experimental Results}

\section{Discussion}
The results demonstrate the effectiveness of using evolutionary strategies (ES) with 1/5 success rule to optimize CNN architectures for steering angle prediction in autonomous driving systems and hence meets our goals. Key findings include:
Baseline Performance: The PilotNET baseline achieved reasonable accuracy but required significant computational resources due to its large model size (245.40 million parameters, 936.44 MB).
This highlights the need for architecture optimization to improve efficiency without sacrificing performance.
Impact of ES Optimization: The ES-optimized PilotNET model achieved the lowest error (MSE: 0.49, MAE: 0.41 degrees) while significantly reducing the model size (50.97 million parameters, 194.45 MB). Models with reduced convolutional layers showed trade-offs: while they consumed fewer resources, their prediction accuracy relatively suffered, with the quarter-size model exhibiting the highest error (MSE: 1.88, MAE: 0.88 degrees). This study shows that using ES to optimize models for better performance or size reduction while does yield practical results that can improve application efficiency.
Real-time Performance: Visual results confirm that the optimized models deliver accurate steering predictions even in complex driving scenarios, with low absolute errors between expected and predicted steering angles. In real-world applications a small difference in error (<1.0 degree) with a major difference in model size (5-25\% of baseline size) is a valid tradeoff.

In the future, we plan to:
Expand the dataset by recording driving data on various roads across the LTU campus, including different surfaces, path geometries, and lighting conditions.
Add Recurrent Neural Network layers to train models based on the past steering inputs
Deploy the optimized CNN models on the LTU ACTor autonomous driving platform.
Evaluate optimized models on low-power compute platforms, such as NVIDIA Jetson or Raspberry Pi, to assess scalability for cost-effective systems.



\section*{Acknowledgment}

The preferred spelling of the word ``acknowledgment'' in America is without
an ``e'' after the ``g''. Avoid the stilted expression ``one of us (R. B.
G.) thanks $\ldots$''. Instead, try ``R. B. G. thanks$\ldots$''. Put sponsor
acknowledgments in the unnumbered footnote on the first page.

\section*{References}

Please number citations consecutively within brackets \cite{}. The
sentence punctuation follows the bracket \cite{}. Refer simply to the reference
number, as in \cite{}---do not use ``Ref. \cite{}'' or ``reference \cite{}'' except at
the beginning of a sentence: ``Reference \cite{} was the first $\ldots$''

Number footnotes separately in superscripts. Place the actual footnote at
the bottom of the column in which it was cited. Do not put footnotes in the
abstract or reference list. Use letters for table footnotes.

Unless there are six authors or more give all authors' names; do not use
``et al.''. Papers that have not been published, even if they have been
submitted for publication, should be cited as ``unpublished'' \cite{}. Papers
that have been accepted for publication should be cited as ``in press'' \cite{}.
Capitalize only the first word in a paper title, except for proper nouns and
element symbols.

For papers published in translation journals, please give the English
citation first, followed by the original foreign-language citation \cite{}.

\bibliographystyle{plain} % We choose the "plain" reference style
\bibliography{references} % Entries are in the references.bib file

\end{document}
b2